# -*- coding: utf-8 -*-
"""Untitled16.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Rs9qYuGJe4SX22Nb2xitcdk8Ud5YZxJZ
"""

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder

!unzip -q "/content/drive/MyDrive/archive (2).zip"

df = pd.read_csv("IRIS.csv")
df.head()

df['species'] = encode.fit_transform(df[['species']])
encode = LabelEncoder()

df.head()

import seaborn as sns
sns.pairplot(df , hue  = 'species')

df = df[df['species'] != 0] [['sepal_length' , 'sepal_width' , 'species']]

X = df.iloc[:,:-1]
y = df.iloc[:,-1]

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

clf1 = LogisticRegression()
clf2 = KNeighborsClassifier()
clf3 = RandomForestClassifier()

estimators = [('lr' , clf1) , ('knn' , clf2) , ('rf' , clf3)]

for estimator in estimators:
    x = cross_val_score(estimator[1] , X , y ,cv = 10, scoring = 'accuracy')
    print(estimator[0] , np.round(np.mean(x) , 2))

from sklearn.ensemble import VotingClassifier
vclf = VotingClassifier(estimators = estimators , voting = 'hard')
x = cross_val_score(vclf , X , y , cv = 10 , scoring = 'accuracy')
print(np.round(np.mean(x) , 2))

from sklearn.ensemble import VotingClassifier
vclf = VotingClassifier(estimators = estimators , voting = 'soft')
x = cross_val_score(vclf , X , y , cv = 10 , scoring = 'accuracy')
print(np.round(np.mean(x) , 2))

for i in range (1,4):
    for j in range(1,4):
        for k in range(1,4):
            vclf = VotingClassifier(estimators = estimators , voting = 'hard' , weights = [i,j,k])
            x = cross_val_score(vclf , X , y , cv = 10 , scoring = 'accuracy')
            print("for i = {} , j = {} , k = {}".format(i,j,k) , np.round(np.mean(x) , 2))

from sklearn.svm import SVC
from sklearn.datasets import make_classification

X , y = make_classification(n_samples = 1000 , n_features = 20 , n_informative = 15 , n_redundant = 5 , random_state = 45)
svc1 = SVC(probability= True , kernel = 'poly' , degree = 1)
svc2 = SVC(probability= True , kernel = 'poly' , degree = 2)
svc3 = SVC(probability= True , kernel = 'poly' , degree = 3)
svc4 = SVC(probability= True , kernel = 'poly' , degree = 4)
svc5 = SVC(probability= True , kernel = 'poly' , degree = 5)

estimators = [('svc1' , svc1) , ('svc2' , svc2) , ('svc3' , svc3) , ('svc4' , svc4) , ('svc5' , svc5)]

for estimator in estimators:
    x = cross_val_score(estimator[1] , X , y ,cv = 10, scoring = 'accuracy')
    print(estimator[0] , np.round(np.mean(x) , 2))

vclf1 = VotingClassifier(estimators = estimators , voting = 'hard')
x = cross_val_score(vclf , X , y , cv = 10 , scoring = 'accuracy')
print(np.round(np.mean(x) , 2))

vclf1 = VotingClassifier(estimators = estimators , voting = 'soft')
x = cross_val_score(vclf , X , y , cv = 10 , scoring = 'accuracy')
print(np.round(np.mean(x) , 2))